<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gradient Descent on Ndukwe Chidubem</title>
    <link>https://Duks31.github.io/tags/gradient-descent/</link>
    <description>Recent content in Gradient Descent on Ndukwe Chidubem</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 11 Aug 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://Duks31.github.io/tags/gradient-descent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Gradient Descent</title>
      <link>https://Duks31.github.io/posts/2023-08-11-understanding-gradient-descent/</link>
      <pubDate>Fri, 11 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://Duks31.github.io/posts/2023-08-11-understanding-gradient-descent/</guid>
      <description>Introduction&#xA;Gradient Descent (GD) is a first order optimization algorithms that is used to find the minimum of a given function, it is commonly used in training machine learning and deep learning algorithms.&#xA;In machine learning after you train a model you would like to check how the model performed either by checking the accuracy or any other measure that suites the problem, letâ€™s say you generate a **cost function **which is a measure of how wrong the model is finding a relationship between the input and the output, this is where Gradient Descent comes in, the gradient descent algorithm will optimize the cost function, it will give the minimum value of possible error of your model.</description>
    </item>
  </channel>
</rss>
